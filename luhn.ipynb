{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "484ad332-4f1d-4483-9a8c-3d0d71e11910",
   "metadata": {},
   "source": [
    "# Imports/Setup\n",
    "\n",
    "Luhn, Hans Peter. \"The Automatic Creation of Literature Abstracts.\" *IBM Journal of Research and Revelopment 2.2* (1958): 159-165.\n",
    "\n",
    "DOI: [10.1147/rd.22.0159](https://ieeexplore.ieee.org/document/5392672)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d84a19cf-3999-4d18-8bf8-db43eeff1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import luhn_abstract as la\n",
    "import requests\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19adde3f-760d-4eed-9e68-53b2425e83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_n = 4\n",
    "val_k = 5\n",
    "val_quant_lower = 0.25\n",
    "val_quant_upper = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5f59a40-c6f9-4c5b-97eb-6aebcd9c429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_wikipedia(val_url):\n",
    "    html_scraped_text = requests.get(val_url)\n",
    "    html_text_parsed = bs.BeautifulSoup(html_scraped_text.content,'lxml')\n",
    "    html_text_content = html_text_parsed.find(id='content')\n",
    "    html_text_paragraphs = html_text_content.find_all('p')\n",
    "    val_text_formatted = ''\n",
    "    for val in html_text_paragraphs:\n",
    "        val_text_formatted += val.text\n",
    "    return(val_text_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8415857-764c-4fc8-8cc5-e6cb2cd17288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a18097d6-c67e-4a74-a497-124fd7d15820",
   "metadata": {},
   "source": [
    "# Examples\n",
    "## Request NLP Wikipedia Summary\n",
    "### Use Luhn Score\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa3ca42c-7523-49af-9875-308d3b61abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1.00001\n",
      "Quantile Significance Upper = 31.0\n",
      "--------------------------------------------------\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[15.6250]\u001b[0m [9] In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. \u001b[31m[14.2258]\u001b[0m It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \u001b[31m[12.9706]\u001b[0m Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \u001b[31m[12.5000]\u001b[0m Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[12.2500]\u001b[0m\n",
      "CPU times: user 636 ms, sys: 28 ms, total: 664 ms\n",
      "Wall time: 880 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/natural_language_processing')\n",
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d310e762-7acc-4394-a6ec-452bd17f5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id_sent</th>\n",
       "      <th>count</th>\n",
       "      <th>significance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>natural</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>language</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>processing</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nlp</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  id_sent  count  significance  score\n",
       "0     natural        0     16            16     16\n",
       "1    language        0     22            22     22\n",
       "2  processing        0     11            11     11\n",
       "3         nlp        0     15            15     15\n",
       "4          is        0     13            13     13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words_scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca380553-6ecb-4097-b898-f0e0e8bff230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_sent</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.0</td>\n",
       "      <td>15.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>14.225806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>12.970588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>12.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.0</td>\n",
       "      <td>12.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_sent      score\n",
       "40     40.0  15.625000\n",
       "13     13.0  14.225806\n",
       "1       1.0  12.970588\n",
       "3       3.0  12.500000\n",
       "18     18.0  12.250000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences_scored.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d8ea17-24c6-4ce1-9f08-2a984eec25f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id_sent</th>\n",
       "      <th>count</th>\n",
       "      <th>significance</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>it</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>primarily</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>concerned</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>with</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  id_sent  count  significance  score\n",
       "14         it        1      3             3      3\n",
       "15         is        1     13            13     13\n",
       "16  primarily        1      1             1      0\n",
       "17  concerned        1      1             1      0\n",
       "18       with        1     16            16     16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words_scored.loc[df_words_scored['id_sent']==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b864f5d-d553-43b1-b595-a4020de204ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67502dcf-706f-4397-b6e7-4a05aa429b51",
   "metadata": {},
   "source": [
    "### Experiment #1: Remove Stop Words\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| Yes | True |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2511b05f-48e4-4e33-a0e2-9c00409f009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1.00001\n",
      "Quantile Significance Upper = 9.0\n",
      "--------------------------------------------------\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[13.5000]\u001b[0m In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. \u001b[31m[8.0667]\u001b[0m Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words. \u001b[31m[7.5789]\u001b[0m [9] In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. \u001b[31m[7.5789]\u001b[0m Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning. \u001b[31m[7.5625]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=True)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a7257-6a54-4508-a7e8-482d5295e284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75f86b7d-dce1-459f-bc8c-b2ae42a3234a",
   "metadata": {},
   "source": [
    "### Experiment #2: Use Mean Score\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297e01ad-5fd8-4aee-b2ef-e9e660e086a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1.00001\n",
      "Quantile Significance Upper = 31.0\n",
      "--------------------------------------------------\n",
      "It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \u001b[31m[6.7941]\u001b[0m Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \u001b[31m[6.6667]\u001b[0m Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. \u001b[31m[6.5000]\u001b[0m Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning. \u001b[31m[6.1818]\u001b[0m [57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[5.6780]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=np.mean)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee528e-cd2b-4802-bbfc-4399e3d3cfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd73e984-65d8-49de-9eb9-34a4339f4262",
   "metadata": {},
   "source": [
    "### Experiment #3: Zero Score Stop Words\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | Yes | True |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21fe7c97-4d0d-4430-a62e-adceeb3ea1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1e-05\n",
      "Quantile Significance Upper = 31.0\n",
      "--------------------------------------------------\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[26.5610]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[22.2308]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[13.2353]\u001b[0m [17] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming. \u001b[31m[12.5652]\u001b[0m Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[10.7037]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=True)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55e026-9ff6-4d97-930e-6e58a2ab1e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f86b663-8ff0-4920-b9b5-e5edf9fd9642",
   "metadata": {},
   "source": [
    "### Experiment #4: Use TF Counting\n",
    "\n",
    "$$tf=\\frac{f_t}{\\sum{f_t}}$$\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d9c9723-5754-4840-957e-b1ca1e0e545b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0.0008713264427217916\n",
      "Quantile Significance Upper = 0.026701119724375538\n",
      "--------------------------------------------------\n",
      "It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \u001b[31m[6.7941]\u001b[0m Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \u001b[31m[6.6667]\u001b[0m Natural language processing (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. \u001b[31m[6.5000]\u001b[0m Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning. \u001b[31m[6.1818]\u001b[0m [57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[5.6780]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=False,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=np.mean)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da013bdf-a2c1-4ea2-ad02-2ce1b60da858",
   "metadata": {},
   "source": [
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6914f08-58dd-4cfb-95bd-420fe53e3c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0.0008713264427217916\n",
      "Quantile Significance Upper = 0.026701119724375538\n",
      "--------------------------------------------------\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[15.6250]\u001b[0m [9] In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. \u001b[31m[14.2258]\u001b[0m It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. \u001b[31m[12.9706]\u001b[0m Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation. \u001b[31m[12.5000]\u001b[0m Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[12.2500]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=False,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399aa17-9a00-451c-81cd-9eb5a9480f4c",
   "metadata": {},
   "source": [
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | Yes | True |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64267824-9f66-4161-a345-914467bd0e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1e-05\n",
      "Quantile Significance Upper = 0.026701119724375538\n",
      "--------------------------------------------------\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[10.2712]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[10.1017]\u001b[0m [17] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming. \u001b[31m[5.1600]\u001b[0m Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning. \u001b[31m[4.7727]\u001b[0m [48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. \u001b[31m[4.3333]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=False,is_remove_stopwords=True)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=np.mean)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6851c468-3fd0-4102-b025-75602cf94e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c60dc2d-b8c9-4914-876e-ab86fd089720",
   "metadata": {},
   "source": [
    "## Request George Washington Wikipedia Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a7bf20d-3dcb-4dc8-b95a-ff29f2b095a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 3.0\n",
      "Quantile Significance Upper = 470.0\n",
      "--------------------------------------------------\n",
      "[38] Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor as Commander-in-Chief, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. \u001b[31m[31.1136]\u001b[0m While his relationship with Washington would remain friendly, Washington's relationship with his Secretary of War Henry Knox deteriorated after rumors that Knox had profited from contracts for the construction of U.S. frigates which had been commissioned under the Naval Act of 1794 in order to combat Barbary pirates, forcing Knox to resign. \u001b[31m[29.7609]\u001b[0m Appointed by the Second Continental Congress as commander of the Continental Army in 1775, Washington led Patriot forces to victory in the American Revolutionary War and then served as president of the Constitutional Convention in 1787, which drafted the current Constitution of the United States. \u001b[31m[25.9730]\u001b[0m Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on \"a shore foreign to his native soil\" if the country became divided, and Washington's remains stayed in Mount Vernon. \u001b[31m[25.3256]\u001b[0m [52] The marriage gave Washington control over Martha's one-third dower interest in the 18,000-acre (7,300 ha) Custis estate, and he managed the remaining two-thirds for Martha's children; the estate also included 84 slaves. \u001b[31m[25.2895]\u001b[0m\n",
      "CPU times: user 8.64 s, sys: 66.4 ms, total: 8.71 s\n",
      "Wall time: 8.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/George_Washington')\n",
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f06c3-5efb-42ff-b02c-5912bfd1e145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5cce7f0-b43c-4790-a06c-910bb05753b0",
   "metadata": {},
   "source": [
    "## Request GMU Wikipedia Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d46124-8c08-41bd-b288-518753419888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 2.0\n",
      "Quantile Significance Upper = 127.0\n",
      "--------------------------------------------------\n",
      "8 in the nation for Freedom of Speech and protecting rights enshrined in the First Amendment to the United States Constitution according to the Foundation for Individual Rights and Expression (FIRE), FIRE also posits that the majority viewpoint of the student body leans politically liberal in the sense of modern liberalism in the United States,[138][139] although the political ideologies of libertarianism in the United States[140][141][142] and conservatism in the United States[143][144] are also visible on campus with the university stating that it strives for \"comprehensive ideological balance,\" evidence including but not limited to the university being \"home to both the Antonin Scalia Law School and the Jimmy and Rosalynn Carter School for Peace and Conflict Resolution,\" named after a conservative U.S. Supreme Court Justice (Antonin Scalia) politically appointed by Republican Party U.S. President and a liberal U.S. President (Jimmy Carter) and First Lady (Rosalynn Carter) who are members of the Democratic Party, respectively. \u001b[31m[42.9206]\u001b[0m Among undergraduate students, 80% of students are enrolled full-time while 20% are enrolled part-time[146] In terms of ethnic and racial demographics: American Indian/Alaska Native people make up 0% of the student body and 0% of the full-time staff; Asian people make up 22% of the student body and 14% of the full-time staff; Black people make up 11% of the student body and 5% of the full-time staff; Hispanic and Latino people make up 17% of the student population and 3% of the full-time staff; Native Hawaiian/Pacific Islander people make up 0% of the student body and 0% of the full-time staff; non-resident alien people make up 5% of the student body and 8% of the full-time staff; people of two or more races/multiracial people make up 5% of the student body and 2% of the full-time staff; people of an Unknown ethno-racial demographic make up 3% of the student body and 3% of the full-time staff; and White people make up 36% of the student body and 65% of the full-time staff. \u001b[31m[34.9355]\u001b[0m [12] Originally founded as a branch of the University of Virginia, the university has since expanded into a residential college for traditional students with an emphasis on combining modern practice-based professional education with a comprehensive traditional liberal arts curriculum while maintaining its historic commuter student-inclusive environment at both undergraduate and post-graduate levels. \u001b[31m[34.2407]\u001b[0m [55] The Arlington Campus, named Mason Square in 2022,[56] is situated on 5.2 acres (2.1 hectares) in Virginia Square, an urban environment on the edge of Arlington County, Virginia's Clarendon business district and four miles (6.4 km) from downtown Washington, D.C. \u001b[31m[32.0889]\u001b[0m [45] Opened in March 2014, Mason Korea is located in the Songdo International Business District in South Korea, a 42,000-acre (17,000-hectare) site designed for 850,000 people. \u001b[31m[27.1290]\u001b[0m\n",
      "CPU times: user 2.86 s, sys: 30.2 ms, total: 2.89 s\n",
      "Wall time: 3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/George_Mason_University')\n",
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010538df-d7ac-44ff-a0f0-e54afdd87ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de4a79cd-64e4-40c8-bb78-75f19d82d469",
   "metadata": {},
   "source": [
    "# Random Example\n",
    "\n",
    "*Source: Scientific American*  \n",
    "*Title: A New Quantum Cheshire Cat Thought Experiment Is Out of the Box*  \n",
    "*Author: Manon Bischoff*\n",
    "\n",
    "[A New Quantum Cheshire Cat Thought Experiment Is Out of the Box](https://www.scientificamerican.com/article/a-new-quantum-cheshire-cat-thought-experiment-is-out-of-the-box/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "494a2d7a-d103-495b-b827-9c513311aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = '''Physicists seem to be obsessed with cats. James Clerk Maxwell, the father of electrodynamics, studied falling felines to investigate how they turned as they fell. Many physics teachers have used a cat’s fur and a hard rubber rod to explain the phenomenon of frictional electricity. And Erwin Schrödinger famously illustrated the strangeness of quantum physics with a thought experiment involving a cat that is neither dead nor alive.  So it hardly seems surprising that physicists turned to felines once again to name a newly discovered quantum phenomenon in a paper published in the New Journal of Physics in 2013. Their three-sentence study abstract reads, “In this paper we present a quantum Cheshire Cat. In a pre- and post-selected experiment we find the Cat in one place, and its grin in another. The Cat is a photon, while the grin is its circular polarization.”\n",
    "The newfound phenomenon was one in which certain particle features take a different path from their particle—much like the smile of the Cheshire Cat in Alice’s Adventures in Wonderland, written by Lewis Carroll—a pen name of mathematician Charles Lutwidge Dodgson—and published in 1865. To date, several experiments have demonstrated this curious quantum effect. But the idea has also drawn significant skepticism. Critics are less concerned about the theoretical calculations or experimental rigor than they are about the interpretation of the evidence. “It seems a bit bold to me to talk about disembodied transmission,” says physicist Holger Hofmann of Hiroshima University in Japan. “Instead we should revise our idea of particles.”  Recently researchers led by Yakir Aharonov of Chapman University took the debate to the next level. Aharonov was a co-author of the first paper to propose the quantum Cheshire effect. Now, on the preprint server arXiv.org, he and his colleagues have posted a description of theoretical work that they believe demonstrates that quantum properties can move without any particles at all—like a disembodied grin flitting through the world and influencing its surroundings—in ways that bypass the critical concerns raised in the past.\n",
    "Aharonov and his colleagues first encountered their quantum Cheshire cat several years ago as they were pondering one of the most fundamental principles of quantum mechanics: nothing can be predicted unambiguously. Unlike classical physics, the same quantum mechanical experiment can have different outcomes under exactly the same conditions. It is therefore impossible to predict the exact outcome of a single experiment—only its outcome with a certain probability. “Nobody understands quantum mechanics. It’s so counterintuitive. We know its laws, but we are always surprised,” says Sandu Popescu, a physicist at the University of Bristol in England, who collaborated with Aharonov on the 2013 paper and the new preprint.\n",
    "But Aharonov was not satisfied with this uncertainty. So, since the 1980s, he has been exploring ways to investigate fundamental processes despite the probability-based nature of quantum mechanics. Aharonov—now age 92—employs an approach that involves intensively repeating an experiment, grouping results and then examining what came out before and after the experiment and relating these events to each other. “To do this, you have to understand the flow of time in quantum mechanics,” Popescu explains. “We developed a completely new method to combine information from measurements before and after the experiment.”\n",
    "The researchers have stumbled across several surprises with this method—including their theoretical Cheshire cat. Their idea sounds simple at first: send particles through an optical tool called an interferometer, which causes each particle to move through one of two paths that ultimately merge again at the end. If the setup and measurements were carried out skillfully, Aharonov and his colleagues theorized, it could be shown that the particle traveled a path in the interferometer that differed from the path of its polarization. In other words, they claimed the property of the particle could be measured on one path even though the particle itself took the other—as if the grin and the cat had come apart.\n",
    "Inspired by this theory, a team led by Tobias Denkmayr, then at the Vienna University of Technology, implemented the experiment with neutrons in a study published in 2014. The team showed that the neutral particles inside an interferometer followed a different path from that of their spin, a quantum mechanical property of particles similar to angular momentum: Denkmayr and his colleagues had indeed found evidence of the Cheshire cat theory. Two years later researchers led by Maximilian Schlosshauer of the University of Portland successfully implemented the same experiment with photons. The scientists saw evidence that the light particles took a different path in the interferometer than their polarization did.\n",
    "But not everyone is convinced. “Such a separation makes no sense at all. The location of a particle is itself a property of the particle,” Hofmann says. “It would be more accurate to talk about an unusual correlation between location and polarization.” Last November Hofmann and his colleagues provided an alternative explanation based on widely known quantum mechanical effects.\n",
    "And in another interpretation of the Cheshire cat results, Pablo Saldanha of the Federal University of Minas Gerais in Brazil and his colleagues argue that the findings can be explained with wave-particle duality. “If you take a different view, there are no paradoxes,” Saldanha says, “but all results can be explained with traditional quantum mechanics as simple interference effects.”\n",
    "Much of the controversy surrounds the way in which particles’ properties and positions are detected in these experiments. Disturbing a particle could alter its quantum mechanical properties. For that reason, the photons or neutrons cannot be recorded inside the interferometer using an ordinary detector. Instead scientists must resort to a principle of weak measurement developed by Aharonov in 1988. A weak measurement makes it possible to scan a particle very lightly without destroying its quantum state. This comes at a price, however: the weak measurement result is extremely inaccurate. (Thus, these experiments must be repeated many times over, to compensate for the fact that each individual measurement is highly uncertain.)\n",
    "In the quantum Cheshire cat experiments, a weak measurement is made along a path in the interferometer, the paths then merge, and the emerging particles are measured with an ordinary detector. Along one path of the interferometer, a weak measurement of the particle’s position can be taken and, along the other, its spin. Using detectors, physicists can more definitively characterize the particles that traveled through the interferometer and potentially reconstruct what occurred during the particle’s journey. For example, only certain particles will appear in certain detectors, helping the physicists piece together which path their neutron or photon previously took. According to Aharonov, Popescu and their colleagues, the Cheshire cat experiments ultimately reveal that the particle’s position can be confirmed on one path even as its polarization or spin was measured on the other.\n",
    "Saldanha and his co-authors assert that it is impossible to make claims about quantum systems in the past given their measurements in the present. In other words, the photons and neutrons measured in the final detectors cannot tell us much about their previous trajectory. Instead the wave functions of particles passing through the paths of the interferometer could overlap, which would make it impossible to trace which path a particle had taken. “Ultimately, the paradoxical behaviors are related to the wave-particle duality,” Saldanha says. But in the papers that report evidence of the quantum Cheshire cat, he asserts, the findings “are processed in a sophisticated way that obscures this simpler interpretation.”\n",
    "Hofmann, meanwhile, has stressed that the results will differ if you measure the system in a different way. This phenomenon is well-known in quantum physics: if, for example, you first measure the speed of a particle and then its position, the result can be different than it would be if you first measured the position of the same particle and then its speed. He and his colleagues therefore contend that Aharonov and his team’s conclusions were correct in themselves—that the particle moved along one path and the polarization followed the other—but that such differing paths do not apply simultaneously.\n",
    "As Hofmann’s co-author Jonte Hance, also at Hiroshima University, told New Scientist, “It only looks like [the particle and polarization are] separated because you’re measuring one of the properties in one place and the other property in the other place, but that doesn’t mean that the properties are in one place and the other place, that means that the actual measuring itself is affecting it in such a way that it looks like it’s in one place and the other place.”\n",
    "But these critiques are “missing the point,” Popescu says. He agrees that the work and reasoning put forward by Saldanha and Hofmann’s respective groups are correct—but adds that the best way to test any interpretation is to generate testable predictions from each. “As I understand it, there is no direct way to make predictions based on them,” Popescu says in reference to these alternative explanations. “They kind of have a very old-fashioned way of looking at things: there are contradictions, so you stop doing the math.”\n",
    "With their recent preprint paper, Aharonov and Popescu, together with physicist Daniel Collins of the University of Bristol, have now described how a particle’s spin can move completely independently of the particle itself—without employing a weak measurement. In their new experimental setup, a particle is located in the left half of an elongated two-part cylinder that is sealed at the outer edges. Because of a highly reflective wall in the middle, the particle has a vanishingly small probability of tunneling through to the right-hand side of the cylinder. In their paper, the researchers provide a proof that even if the particle remains in the left-hand area in almost all cases, it should still be possible to measure a transfer of the particle’s spin at the right-hand outer wall. “It’s amazing, isn’t it?” Collins says. “You think the particle has a spin and the spin should stay with the particle. But the spin crosses the box without the particle.”\n",
    "This approach would address several of the critical concerns raised thus far. The physicists don't need weak measurements. Nor do they need to group their experimental results to draw temporal conclusions. (That being said, grouping results would still improve the measurements, given that the angular momentum of the wall itself cannot be determined unambiguously because of the Heisenberg uncertainty principle.) But in this scenario, the only physical principles involved are conservation laws, such as the conservation of energy or the conservation of momentum and angular momentum. Popescu and Collins explain that they hope other groups will implement the experiment to observe the effects in the laboratory.\n",
    "The new work has piqued Hofmann’s interest. “The scenario is exciting because the interaction between polarization and particle motion produces a particularly strong quantum effect that clearly contradicts the particle picture,” he says.\n",
    "But he still does not see this as proof of disembodied (particle-free) spin transfer. “For me, this means, above all, that it is wrong to assume a measurement-independent reality,” Hofmann says. Instead quantum mechanics allows a particle’s residence to extend to the right-hand region of the cylinder, even if a residence in the left-hand region seems logically compelling. “I think it is quite clear to Aharonov, Collins and Popescu that the space in front of the wall is not really empty,” he adds.\n",
    "Saldanha, meanwhile, still sees the researchers as overcomplicating what could be explained as traditional quantum interference effects. When discussing the particle’s very low probability of entering the right-hand side of the experimental setup, he explains, “we have to be careful about a ‘vanishingly small probability’ when we refer to waves.” The wave function of the particle could also expand into the right-hand side of the setup and thus influence the angular momentum of the wall. “The same predictions can be made without such dramatic conclusions,” he says.\n",
    "In response to these critiques, Popescu says, “This is of course another way of thinking about it. The question is whether this interpretation is useful.” Regardless of which interpretation of the events is correct, the quantum Cheshire cat could enable new technological applications. For example, it could be used to transfer information or energy without moving a physical particle—whether made of matter or light.\n",
    "For Popescu, however, the fundamental questions of physics play a more important role. “It all started when we thought about how time propagates in quantum mechanics,” he says. “And suddenly we were able to discover something fundamental about the laws of conservation.”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e5f4fd-efc2-4a89-ac3f-b29e0636b76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 2.0\n",
      "Quantile Significance Upper = 56.0\n",
      "--------------------------------------------------\n",
      "As Hofmann’s co-author Jonte Hance, also at Hiroshima University, told New Scientist, “It only looks like [the particle and polarization are] separated because you’re measuring one of the properties in one place and the other property in the other place, but that doesn’t mean that the properties are in one place and the other place, that means that the actual measuring itself is affecting it in such a way that it looks like it’s in one place and the other place.” But these critiques are “missing the point,” Popescu says. \u001b[31m[54.4444]\u001b[0m This phenomenon is well-known in quantum physics: if, for example, you first measure the speed of a particle and then its position, the result can be different than it would be if you first measured the position of the same particle and then its speed. \u001b[31m[31.3913]\u001b[0m According to Aharonov, Popescu and their colleagues, the Cheshire cat experiments ultimately reveal that the particle’s position can be confirmed on one path even as its polarization or spin was measured on the other. \u001b[31m[24.7353]\u001b[0m He and his colleagues therefore contend that Aharonov and his team’s conclusions were correct in themselves—that the particle moved along one path and the polarization followed the other—but that such differing paths do not apply simultaneously. \u001b[31m[24.3243]\u001b[0m “If you take a different view, there are no paradoxes,” Saldanha says, “but all results can be explained with traditional quantum mechanics as simple interference effects.” Much of the controversy surrounds the way in which particles’ properties and positions are detected in these experiments. \u001b[31m[21.3333]\u001b[0m\n",
      "CPU times: user 1.41 s, sys: 22.6 ms, total: 1.43 s\n",
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc820fb-32ea-43c2-baaa-50b2995083a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08454489-c490-4af0-ac7e-a70cee027d9f",
   "metadata": {},
   "source": [
    "# Exhibit 2\n",
    "*Source: The New York Times, September8, 1957, page E11*  \n",
    "*Title: Chemistry Is Employed in a Search for New Methods toConquer Mental Illness*  \n",
    "*Author: Robert K. Plumb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cda2110-73f8-4d22-843b-c435df6a6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = '''By coincidence this weekend in New York City marks the end of the annual meeting of the American Psychological Association and the begining of \n",
    "the annual meeting of the American Chemical Society. Psychologists and chemists have never had so much in common as they now have in new studies of the chemical \n",
    "basis for human behavior. Exciting new finds in this field were also discussed last week in Iowa City, Iowa, at the annual meeting of the American \n",
    "Physiological Society and at Zurich, Switzerland, at the Second International Congress for Psychiatry. Two major recent developments have called the attention \n",
    "of chemists, physiologists, physicists and other scientists to mental diseases: It has been found that extremely minute quantities of chemicals can induce \n",
    "hallucinations and bizarre psychic disturbances in normal people, and mood-altering drugs (tranquilizers, for instance) have made long-institutionalized people \n",
    "amenable to therapy. Money to finance resreach on the physical factors in mental illness is being made available. Progress has \n",
    "been achieved toward the understanding of the chemistry of the brain. New goals are in sight. At the psychiatrists meeting in Zurich last week, four New York \n",
    "City physicians urged their colleagues to broaden their concept of \"mental disease,\" and to probe more deeply into the chemistry and metabolism of the human \n",
    "body for answers to mental disorders and their prevention. Dr. Felix Marti-Ibanez and three brothers, Dr. Mortimer D. Sackler, \n",
    "Dr. Raymond R. Sackler and Dr. Arthur M. Sackler cited evidence that the blood chemistry of victims of schizophrenia is different from that of normal people. \n",
    "Perhaps multiple biological factors are responsible for this chemical change, they suggested. Mental disease is a \"developmental process\" and long duration of \n",
    "a disorder may result in \"permanent alteration of anatomy and physiology,\" they said. They urged that trials of new drugs which affect the brain should be \n",
    "concentrated on complex studies of the mechanism of action of the drugs. The variety of substances capable of producing profound mental effects is a new \n",
    "armory of weapons for use in investigating biological mechanisms underlying mental disease, they said. The sources of behavioral disturbance are many and they \n",
    "may come from external as well as internal forces, the four reported. This concept has already proven practical, for instances, when it enabled psychiatrists \n",
    "to predict that the administration of ACTH and cortisone could produce psychosis. \"It led some years ago to the development of a blood test which was 80 percent \n",
    "accurate in the identification of schizophrenic patients,\" they said. \"It permitted us on physiologic grounds to deny that the psychoneuroses and the \n",
    "psychoses were lesser and greater degrees of the same disease process, and, in fact, to affirm that they represented opposite and even mutually exclusive \n",
    "directions of physiologic disturbances,\" they said. Chemicals now available should he used not only to bring relief to the mentally sick but also to uncover \n",
    "the biological mechanisms of the disease processes themselves. \"Only then will the metabolic era mature and bring to fruition man's long hoped for salvation \n",
    "from the ravages of mental disease,\" they reported.\n",
    "At the psychologist's meeting here, a technique for tracing electrical activity in specific portions of the animal brain was described by researchers from the \n",
    "University of California at Los Angeles. They reported that deep brain implants in cat brains were used to record electrical discharges created as the animals \n",
    "respond to stimulations to which they had been conditioned. In this way the California group reported, it is possible to track the sequence in which the brain \n",
    "brings its various parts into play in learning. Specific areas of memory in the brain may be located. Furthermore, the electrical pathways so traced out can \n",
    "be blocked temporarily by the use of chemicals. This poses new possibilities for studying brain chemistry changes in health and sickness and their alleviation, \n",
    "the California researchers emphasized. The new studies of brain chemistry have provided practical therapeutic results and tremendous encouragement to those who \n",
    "must care for mental patients. One evidence that knowledge in the interdisciplinary field is accumulating fast came last week in an announcement from Washington. \n",
    "This was the establishment by the National Institute of Mental Health of a clearing house of information on psychopharmacology. Literature in the field will be \n",
    "classified and coded so that staff members can answer a wide variety bf technical and scientific questions. People working in the field are invited to send three \n",
    "copies of papers or other material — even informal letters describing work they may have in progress to the Technical Information Unit of the center In \n",
    "Silver Spring, Md.'''\n",
    "val_text = val_text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d06bf9ec-c5a2-403d-a4db-763f484deddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1.00001\n",
      "Quantile Significance Upper = 26.0\n",
      "--------------------------------------------------\n",
      "At the psychiatrists meeting in Zurich last week, four New York City physicians urged their colleagues to broaden their concept of \"mental disease,\" and to probe more deeply into the chemistry and metabolism of the human body for answers to mental disorders and their prevention. \u001b[31m[16.0000]\u001b[0m Exciting new finds in this field were also discussed last week in Iowa City, Iowa, at the annual meeting of the American Physiological Society and at Zurich, Switzerland, at the Second International Congress for Psychiatry. \u001b[31m[15.7500]\u001b[0m Psychologists and chemists have never had so much in common as they now have in new studies of the chemical basis for human behavior. \u001b[31m[11.6364]\u001b[0m Two major recent developments have called the attention of chemists, physiologists, physicists and other scientists to mental diseases: It has been found that extremely minute quantities of chemicals can induce hallucinations and bizarre psychic disturbances in normal people, and mood-altering drugs (tranquilizers, for instance) have made long-institutionalized people amenable to therapy. \u001b[31m[9.0000]\u001b[0m \"Only then will the metabolic era mature and bring to fruition man's long hoped for salvation from the ravages of mental disease,\" they reported.At the psychologist's meeting here, a technique for tracing electrical activity in specific portions of the animal brain was described by researchers from the University of California at Los Angeles. \u001b[31m[8.0000]\u001b[0m\n",
      "CPU times: user 309 ms, sys: 8.57 ms, total: 318 ms\n",
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_sents,df_words = la.luhn_abstract.tokenize(val_text=val_text,is_remove_stopwords=False)\n",
    "df_words_scored = la.luhn_abstract.calc_signifcance_words(df=df_words,is_use_luhn_tf=True,is_remove_stopwords=False)\n",
    "val_sig_lower,val_sig_upper = la.luhn_abstract.word_freq_cutoffs(df=df_words_scored,val_lower=val_quant_lower,val_upper=val_quant_upper)\n",
    "df_words_scored['score'] = [la.luhn_abstract.calc_word_score(val_sig=x,val_lower=val_sig_lower,val_upper=val_sig_upper) for x in df_words_scored['significance']]\n",
    "df_sentences_scored = la.luhn_abstract.calc_sentence_score_all(df=df_words_scored,val_num_apart=val_n,func_summary=None)\n",
    "vec_scores,vec_sents = la.luhn_abstract.summarize(df_sentences=df_sents,df_scores=df_sentences_scored,val_num_sentences=val_k)\n",
    "print('-'*50)\n",
    "str_rtn = la.luhn_abstract.print_summary(vec_scores=vec_scores,vec_sentences=vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b283bc-762f-46c0-8130-18e83a16e25f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f6ee875-3665-4d41-8473-27c9b59c4486",
   "metadata": {},
   "source": [
    "# Potential Issues...\n",
    "\n",
    "- Parsing Wikipedia with NLTK results in sentences that are sometimes more than one independent clause/sentence.\n",
    "- Reproducing the results from the paper is complicated by the fact that exact values for *C* and *D* are not documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51e17b-cf4f-49eb-a2c2-be5580b32d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
