{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "484ad332-4f1d-4483-9a8c-3d0d71e11910",
   "metadata": {},
   "source": [
    "# Effort #1: Recreating Base\n",
    "\n",
    "Luhn, Hans Peter. \"The Automatic Creation of Literature Abstracts.\" *IBM Journal of Research and Revelopment 2.2* (1958): 159-165.\n",
    "\n",
    "DOI: [10.1147/rd.22.0159](https://ieeexplore.ieee.org/document/5392672)\n",
    "\n",
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c811ce98-16a1-4148-ae72-c3ce057e25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luhn_abstract as la"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f77ddc-515c-46d0-bed2-62a44a999685",
   "metadata": {},
   "source": [
    "## Exhibit 2\n",
    "*Source: The New York Times, September8, 1957, page E11*  \n",
    "*Title: Chemistry Is Employed in a Search for New Methods toConquer Mental Illness*  \n",
    "*Author: Robert K. Plumb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3687020-7c1e-4de0-bfe5-cb4c0355575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = '''By coincidence this weekend in New York City marks the end of the annual meeting of the American Psychological Association and the begining of \n",
    "the annual meeting of the American Chemical Society. Psychologists and chemists have never had so much in common as they now have in new studies of the chemical \n",
    "basis for human behavior. Exciting new finds in this field were also discussed last week in Iowa City, Iowa, at the annual meeting of the American \n",
    "Physiological Society and at Zurich, Switzerland, at the Second International Congress for Psychiatry. Two major recent developments have called the attention \n",
    "of chemists, physiologists, physicists and other scientists to mental diseases: It has been found that extremely minute quantities of chemicals can induce \n",
    "hallucinations and bizarre psychic disturbances in normal people, and mood-altering drugs (tranquilizers, for instance) have made long-institutionalized people \n",
    "amenable to therapy. Money to finance resreach on the physical factors in mental illness is being made available. Progress has \n",
    "been achieved toward the understanding of the chemistry of the brain. New goals are in sight. At the psychiatrists meeting in Zurich last week, four New York \n",
    "City physicians urged their colleagues to broaden their concept of \"mental disease,\" and to probe more deeply into the chemistry and metabolism of the human \n",
    "body for answers to mental disorders and their prevention. Dr. Felix Marti-Ibanez and three brothers, Dr. Mortimer D. Sackler, \n",
    "Dr. Raymond R. Sackler and Dr. Arthur M. Sackler cited evidence that the blood chemistry of victims of schizophrenia is different from that of normal people. \n",
    "Perhaps multiple biological factors are responsible for this chemical change, they suggested. Mental disease is a \"developmental process\" and long duration of \n",
    "a disorder may result in \"permanent alteration of anatomy and physiology,\" they said. They urged that trials of new drugs which affect the brain should be \n",
    "concentrated on complex studies of the mechanism of action of the drugs. The variety of substances capable of producing profound mental effects is a new \n",
    "armory of weapons for use in investigating biological mechanisms underlying mental disease, they said. The sources of behavioral disturbance are many and they \n",
    "may come from external as well as internal forces, the four reported. This concept has already proven practical, for instances, when it enabled psychiatrists \n",
    "to predict that the administration of ACTH and cortisone could produce psychosis. \"It led some years ago to the development of a blood test which was 80 percent \n",
    "accurate in the identification of schizophrenic patients,\" they said. \"It permitted us on physiologic grounds to deny that the psychoneuroses and the \n",
    "psychoses were lesser and greater degrees of the same disease process, and, in fact, to affirm that they represented opposite and even mutually exclusive \n",
    "directions of physiologic disturbances,\" they said. Chemicals now available should he used not only to bring relief to the mentally sick but also to uncover \n",
    "the biological mechanisms of the disease processes themselves. \"Only then will the metabolic era mature and bring to fruition man's long hoped for salvation \n",
    "from the ravages of mental disease,\" they reported.\n",
    "At the psychologist's meeting here, a technique for tracing electrical activity in specific portions of the animal brain was described by researchers from the \n",
    "University of California at Los Angeles. They reported that deep brain implants in cat brains were used to record electrical discharges created as the animals \n",
    "respond to stimulations to which they had been conditioned. In this way the California group reported, it is possible to track the sequence in which the brain \n",
    "brings its various parts into play in learning. Specific areas of memory in the brain may be located. Furthermore, the electrical pathways so traced out can \n",
    "be blocked temporarily by the use of chemicals. This poses new possibilities for studying brain chemistry changes in health and sickness and their alleviation, \n",
    "the California researchers emphasized. The new studies of brain chemistry have provided practical therapeutic results and tremendous encouragement to those who \n",
    "must care for mental patients. One evidence that knowledge in the interdisciplinary field is accumulating fast came last week in an announcement from Washington. \n",
    "This was the establishment by the National Institute of Mental Health of a clearing house of information on psychopharmacology. Literature in the field will be \n",
    "classified and coded so that staff members can answer a wide variety bf technical and scientific questions. People working in the field are invited to send three \n",
    "copies of papers or other material — even informal letters describing work they may have in progress to the Technical Information Unit of the center In \n",
    "Silver Spring, Md.'''\n",
    "val_text = val_text.replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d42fcb-4f5d-49f3-8738-31f7a9c990da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 4\n",
      "Exciting new finds in this field were also discussed last week in Iowa City, Iowa, at the annual meeting of the American Physiological Society and at Zurich, Switzerland, at the Second International Congress for Psychiatry. \u001b[31m[10.8889]\u001b[0m \"Only then will the metabolic era mature and bring to fruition man's long hoped for salvation from the ravages of mental disease,\" they reported.At the psychologist's meeting here, a technique for tracing electrical activity in specific portions of the animal brain was described by researchers from the University of California at Los Angeles. \u001b[31m[8.5000]\u001b[0m People working in the field are invited to send three copies of papers or other material — even informal letters describing work they may have in progress to the Technical Information Unit of the center In Silver Spring, Md. \u001b[31m[8.0476]\u001b[0m At the psychiatrists meeting in Zurich last week, four New York City physicians urged their colleagues to broaden their concept of \"mental disease,\" and to probe more deeply into the chemistry and metabolism of the human body for answers to mental disorders and their prevention. \u001b[31m[7.5625]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "luhn_rtn = la.luhn_abstract.run_auto_summarization(val_text=val_text,is_print=True,func_stem_selected=la.luhn_abstract.tokenize_stem_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6a8a80-2553-469e-9441-7d70f362c0f3",
   "metadata": {},
   "source": [
    "## Effort #1: Explore the Parameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f91eda9-04ac-4f3f-a1ee-56917320fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b7cdd79-8465-4008-b124-4b57b4aa312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_quant_lower = np.arange(start=5,stop=31)\n",
    "vec_quant_upper = np.arange(start=30,stop=56)\n",
    "vec_num_apart = np.arange(start=4,stop=8)\n",
    "vec_article_sent_id = [3,23,24]\n",
    "vec_article_send_score = [4.0,5.4,5.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaab3108-da64-48de-9908-0015f9c7dde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 26/26 [41:09<00:00, 94.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40min 42s, sys: 23 s, total: 41min 5s\n",
      "Wall time: 41min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_num_sents = len(vec_article_sent_id)\n",
    "#Setup storage for iteration results...\n",
    "val_cnt = 0\n",
    "vec_iter_sent_ids = []\n",
    "vec_iter_sent_scores = []\n",
    "vec_iter_common_ids = []\n",
    "vec_iter_bound_lower = []\n",
    "vec_iter_bound_upper = []\n",
    "vec_iter_sw_remove = []\n",
    "vec_iter_sw_zero = []\n",
    "vec_iter_stem = []\n",
    "vec_iter_luhn_sw = []\n",
    "vec_iter_num_apart = []\n",
    "#Create iterations over test conditions...\n",
    "for val_lower in tqdm(vec_quant_lower):\n",
    "    for is_sw_remove in [True,False]:\n",
    "        for is_sw_zero in [True,False]:\n",
    "            for func_stem_selected in [None,la.luhn_abstract.tokenize_stem_nltk,la.luhn_abstract.tokenize_stem_luhn]:\n",
    "                for vec_ignore_words in [[],la.luhn_abstract.vec_luhn_sw]:\n",
    "                    for val_num_apart in vec_num_apart:\n",
    "                        for val_upper in vec_quant_upper:\n",
    "                            if(val_lower < val_upper):\n",
    "                                luhn_rtn = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                                                   val_lower_int=val_lower,\n",
    "                                                                                   val_upper_int=val_upper,\n",
    "                                                                                   val_spacing=val_num_apart,\n",
    "                                                                                   val_n=val_num_sents,\n",
    "                                                                                   is_sw_remove=is_sw_remove,\n",
    "                                                                                   is_sw_zero=is_sw_zero,\n",
    "                                                                                   is_use_luhn_tf=True,\n",
    "                                                                                   vec_sw_luhn=vec_ignore_words,\n",
    "                                                                                   vec_sw_add=[],\n",
    "                                                                                   func_stem_selected=func_stem_selected,\n",
    "                                                                                   func_summary_selected=None,\n",
    "                                                                                   is_print=False)\n",
    "                                if(luhn_rtn[1].shape[0]>0):\n",
    "                                    vec_tmp_ids = luhn_rtn[1]['id_sent'].iloc[0:val_num_sents].values.tolist()\n",
    "                                    vec_iter_sent_ids.append(vec_tmp_ids)\n",
    "                                    vec_iter_sent_scores.append(luhn_rtn[1]['score'].iloc[0:val_num_sents].values.tolist())\n",
    "                                    vec_iter_common_ids.append(len(set(vec_tmp_ids)&set(vec_article_sent_id)))\n",
    "                                    del(vec_tmp_ids)\n",
    "                                    vec_iter_bound_lower.append(val_lower)\n",
    "                                    vec_iter_bound_upper.append(val_upper)\n",
    "                                    vec_iter_sw_remove.append(is_sw_remove)\n",
    "                                    vec_iter_sw_zero.append(is_sw_zero)\n",
    "                                    vec_iter_stem.append(func_stem_selected.__name__ if func_stem_selected else 'None')\n",
    "                                    vec_iter_luhn_sw.append([]==vec_ignore_words)\n",
    "                                    vec_iter_num_apart.append(val_num_apart)\n",
    "                            val_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43327832-725c-46be-8e95-ede1d97f15d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vec_sent_ids</th>\n",
       "      <th>vec_scores</th>\n",
       "      <th>val_bound_lower</th>\n",
       "      <th>val_bound_upper</th>\n",
       "      <th>val_num_apart</th>\n",
       "      <th>is_sw_remove</th>\n",
       "      <th>is_sw_zero</th>\n",
       "      <th>is_luhn_sw</th>\n",
       "      <th>stem_func</th>\n",
       "      <th>cnt_common</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>[16.0, 23.0, 24.0]</td>\n",
       "      <td>[2.5, 2.0833333333333335, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tokenize_stem_nltk</td>\n",
       "      <td>2</td>\n",
       "      <td>6.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>[16.0, 23.0, 24.0]</td>\n",
       "      <td>[2.5, 2.0833333333333335, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tokenize_stem_nltk</td>\n",
       "      <td>2</td>\n",
       "      <td>6.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>[16.0, 23.0, 24.0]</td>\n",
       "      <td>[2.5, 2.0833333333333335, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tokenize_stem_nltk</td>\n",
       "      <td>2</td>\n",
       "      <td>6.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>[16.0, 23.0, 24.0]</td>\n",
       "      <td>[2.5, 2.0833333333333335, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tokenize_stem_nltk</td>\n",
       "      <td>2</td>\n",
       "      <td>6.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>[16.0, 23.0, 24.0]</td>\n",
       "      <td>[2.5, 2.0833333333333335, 2.0]</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>tokenize_stem_nltk</td>\n",
       "      <td>2</td>\n",
       "      <td>6.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            vec_sent_ids                      vec_scores  val_bound_lower  \\\n",
       "1495  [16.0, 23.0, 24.0]  [2.5, 2.0833333333333335, 2.0]                8   \n",
       "1496  [16.0, 23.0, 24.0]  [2.5, 2.0833333333333335, 2.0]                8   \n",
       "1497  [16.0, 23.0, 24.0]  [2.5, 2.0833333333333335, 2.0]                8   \n",
       "1498  [16.0, 23.0, 24.0]  [2.5, 2.0833333333333335, 2.0]                8   \n",
       "1499  [16.0, 23.0, 24.0]  [2.5, 2.0833333333333335, 2.0]                8   \n",
       "\n",
       "      val_bound_upper  val_num_apart  is_sw_remove  is_sw_zero  is_luhn_sw  \\\n",
       "1495               30              7         False        True        True   \n",
       "1496               31              7         False        True        True   \n",
       "1497               32              7         False        True        True   \n",
       "1498               33              7         False        True        True   \n",
       "1499               34              7         False        True        True   \n",
       "\n",
       "               stem_func  cnt_common       sum  \n",
       "1495  tokenize_stem_nltk           2  6.583333  \n",
       "1496  tokenize_stem_nltk           2  6.583333  \n",
       "1497  tokenize_stem_nltk           2  6.583333  \n",
       "1498  tokenize_stem_nltk           2  6.583333  \n",
       "1499  tokenize_stem_nltk           2  6.583333  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search = pd.DataFrame({'vec_sent_ids':vec_iter_sent_ids,'vec_scores':vec_iter_sent_scores,'val_bound_lower':vec_iter_bound_lower,\n",
    "                          'val_bound_upper':vec_iter_bound_upper,'val_num_apart':vec_iter_num_apart,'is_sw_remove':vec_iter_sw_remove,\n",
    "                          'is_sw_zero':vec_iter_sw_zero,'is_luhn_sw':vec_iter_luhn_sw,'stem_func':vec_iter_stem,'cnt_common':vec_iter_common_ids})\n",
    "df_param_search['sum'] = [sum(x) for x in df_param_search['vec_scores']]\n",
    "df_param_search.sort_values(by=['cnt_common','sum'],ascending=[False,False],inplace=True)\n",
    "df_param_search.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28c6883-d899-48ca-bba2-8775afa8f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resulted in 64,896 possible parameters to evaluate; however, based on the restrictions for $C$ and $D$, only 12,144 summaries were generated\n"
     ]
    }
   ],
   "source": [
    "print(f'''This resulted in {val_cnt:,} possible parameters to evaluate; however, based on the restrictions for $C$ and $D$, only {df_param_search.shape[0]:,} summaries were generated''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c7c29-5189-4933-869e-d3ce6a8e47c6",
   "metadata": {},
   "source": [
    "Q: What does the distribution for the number of matching sentence IDs look like to the baseline sentences IDs from the article (i.e., $[3,23,24]$)?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057f4390-69d4-4d40-a158-f858eb4b2214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cnt_common\n",
       "1    6635\n",
       "0    4621\n",
       "2     888\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search['cnt_common'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b455bbf2-c239-459e-90b2-74984a9001a9",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, how frequently were stopwords removed from the corpus?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5cbf2a-f520-4237-963c-d4dc4261f795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sw_remove\n",
       "False    888\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['is_sw_remove'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd50aeeb-9bd3-477a-b579-d5a0ce0cda5e",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, how frequently were stopword scores zeroed in the corpus?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bc66780-3ac9-42c0-8877-0c763256e903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sw_zero\n",
       "True     456\n",
       "False    432\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['is_sw_zero'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ce4402-65a1-455d-a46c-9d4899d6d42e",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, how frequently were the Luhn stopwords (prepositions, pronouns, articles) removed from the corpus?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29821acf-ddb3-4945-998a-ad12b7df67fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_luhn_sw\n",
       "True    888\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['is_luhn_sw'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cdae53-8a5d-4703-aabf-8939298d5b30",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, how frequently were the different stemming functions utilized?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b21f2c0a-1044-4dc3-81e0-79b88ad08b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stem_func\n",
       "tokenize_stem_nltk    600\n",
       "None                  144\n",
       "tokenize_stem_luhn    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['stem_func'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf089796-f661-4277-89be-32077e38880c",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, what is the distribution for the length of the span (i.e., $n$)?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba0fb5a1-6832-4d97-a745-61eba29df6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "val_num_apart\n",
       "7    312\n",
       "5    288\n",
       "6    288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['val_num_apart'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57df49-fe2f-411f-bcda-00722f1be919",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs and $n==7$, how frequently were stopword scores zeroed in the corpus?  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c58a283a-52ca-492f-aae9-953045e1bc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sw_zero\n",
       "True     168\n",
       "False    144\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2) & (df_param_search['val_num_apart']==7)]['is_sw_zero'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36d88f-38bf-4368-833b-cdf310d1b096",
   "metadata": {},
   "source": [
    "Q: What are the minimum and maximum values for $C$ and $D$ for the different results by the number of matching sentence IDs to the baseline?  This question provides insight into the \"resolving power\" of the words and how $C$ and $D$ can behave in a similar manner to stopwords.  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69832003-3630-4e70-a6ca-fca564e03efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_bound_lower</th>\n",
       "      <th>val_bound_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnt_common</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            val_bound_lower  val_bound_upper\n",
       "cnt_common                                  \n",
       "0                         5               55\n",
       "1                         5               54\n",
       "2                         8               41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.pivot_table(index='cnt_common',values=['val_bound_lower','val_bound_upper'],aggfunc={'val_bound_lower':pd.Series.min,\n",
    "                                                                                                     'val_bound_upper':pd.Series.max})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d8ea9-56c8-46a5-b10b-d56fe7e2521b",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, what sentence IDs were identified?  To reduce rearrangements of the same IDs, the vector of IDs is first sorted for each result and then converted to a string value.  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08a8e6aa-4454-48fc-b0be-ceb0358ac10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vec_sent_ids\n",
       "[0.0, 3.0, 23.0]      576\n",
       "[3.0, 16.0, 23.0]     288\n",
       "[16.0, 23.0, 24.0]     24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = [x.sort() for x in df_param_search['vec_sent_ids']]\n",
    "df_param_search['str_sent_id'] = [','.join([f'{j:n}' for j in x]) for x in df_param_search['vec_sent_ids']]\n",
    "df_param_search.loc[(df_param_search['cnt_common']==2)]['vec_sent_ids'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5084ccd-48a4-471a-b111-57aa1693f01a",
   "metadata": {},
   "source": [
    "Q: For results with 2 matching sentence IDs, and each vector of sentence IDs identified in the previous block, what were the ranges for $C$ and $D$?  This starts to shed light on some of the nuances for identifying all three sentences from the paper with the parameters.  \n",
    "A: &darr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cfb0d72-dd78-434b-a394-0f12be055ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val_bound_lower</th>\n",
       "      <th>val_bound_upper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>str_sent_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0,3,23</th>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16,23,24</th>\n",
       "      <td>8</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             val_bound_lower  val_bound_upper\n",
       "str_sent_id                                  \n",
       "0,3,23                    14               41\n",
       "16,23,24                   8               41"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_param_search.loc[(df_param_search['cnt_common']==2) &\n",
    "                    (df_param_search['is_luhn_sw']==True) &\n",
    "                    (df_param_search['is_sw_remove']==False) &\n",
    "                    (df_param_search['val_num_apart']==7)].pivot_table(index='str_sent_id',values=['val_bound_lower','val_bound_upper'],\n",
    "                                                                       aggfunc={'val_bound_lower':pd.Series.min,'val_bound_upper':pd.Series.max})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e8d560-89c6-42b8-87b2-931dcc6b2110",
   "metadata": {},
   "source": [
    "## Effort #2: Using More Modern NLP\n",
    "\n",
    "This effort uses data from the [CNN/Daily Mail](https://paperswithcode.com/dataset/cnn-daily-mail-1) effort, which provides news articles with short summaries written by the author.  The raw dataset was available from the [cnn-dailymail GitHub](https://github.com/abisee/cnn-dailymail).  The parser written below provides a quick method to extract and partially clean the article text while separting the highlights (summary) and retaining the ID.  This parser was not intended to holistically clean and produce text equivalent to the Stanford CoreNLP library, but rather as a method to more easily obtain the data for comparing automatic summariztion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8aee9547-6d59-40b9-91bc-4231155c11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4049152c-d0e7-4334-b3af-3643aa76ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 92579 articles in total...\n",
      "CPU times: user 268 ms, sys: 331 ms, total: 598 ms\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#This code provides a rough ETL/parser for the CNN/DailyMail content...\n",
    "vec_story_files = glob.glob('./data/*.story')\n",
    "print(f'''There are {len(vec_story_files)} articles in total...''')\n",
    "vec_id = []\n",
    "vec_stories = []\n",
    "vec_highlights = []\n",
    "vec_ids = []\n",
    "for i,val_file_path in enumerate(vec_story_files):\n",
    "    tmp_open = open(val_file_path,'r').readlines()\n",
    "    tmp_story = ''\n",
    "    tmp_highlights = []\n",
    "    is_past_story = False\n",
    "    for line in tmp_open:\n",
    "        line = line.strip()\n",
    "        if(not is_past_story and line.startswith('@h')):\n",
    "            is_past_story = True\n",
    "        if(not line=='' and not is_past_story):\n",
    "            tmp_story = tmp_story+' '+line\n",
    "        elif(not line=='' and not line.startswith('@h')):\n",
    "            tmp_highlights.append(line)\n",
    "    if(tmp_story.find('-LRB- CNN -RRB-')>=0):\n",
    "        vec_story = tmp_story.split('-LRB- CNN -RRB-')\n",
    "        tmp_story = ' '.join(vec_story[1:])\n",
    "    if(0<=tmp_story.find('--')<50):\n",
    "        vec_story = tmp_story.split('--')\n",
    "        tmp_story = ' '.join(vec_story[1:])\n",
    "    vec_ids.append(val_file_path.split('/')[-1].split('.')[0])\n",
    "    vec_stories.append(tmp_story)\n",
    "    vec_highlights.append(tmp_highlights)\n",
    "    if(i>=4999):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d197ae8-601b-4962-a717-5c44c7c15d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5001, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>story</th>\n",
       "      <th>highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>638ba1352bdf405a8f5bd681d7fe5c928686afff</td>\n",
       "      <td>At the start of a big week for the Higgs boso...</td>\n",
       "      <td>[U.S.-based scientists say their data points t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f9f9601180ab3278165d936821e8f145659997f3</td>\n",
       "      <td>acquitted by a Florida jury over the death of...</td>\n",
       "      <td>[Zimmerman posts $ 5,000 bail ; he was accused...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80ec0efb252ec4470aee44482d1e196111b5780b</td>\n",
       "      <td>Zlatan Ibrahimovic scored his third goal in a...</td>\n",
       "      <td>[Barcelona move three points clear of Real Mad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8435150be66ea9792999dfc233cc690f9c2fe2d0</td>\n",
       "      <td>Nobel laureate Norman E. Borlaug , an agricul...</td>\n",
       "      <td>[Borlaug died at the age of 95 from complicati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1444cf4d1832507a29a98529c2cd1a41f0154b52</td>\n",
       "      <td>Louisiana Gov. Bobby Jindal on Monday stood b...</td>\n",
       "      <td>[Louisiana Gov. Bobby Jindal decried `` no-go ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 article_id  \\\n",
       "0  638ba1352bdf405a8f5bd681d7fe5c928686afff   \n",
       "1  f9f9601180ab3278165d936821e8f145659997f3   \n",
       "2  80ec0efb252ec4470aee44482d1e196111b5780b   \n",
       "3  8435150be66ea9792999dfc233cc690f9c2fe2d0   \n",
       "4  1444cf4d1832507a29a98529c2cd1a41f0154b52   \n",
       "\n",
       "                                               story  \\\n",
       "0   At the start of a big week for the Higgs boso...   \n",
       "1   acquitted by a Florida jury over the death of...   \n",
       "2   Zlatan Ibrahimovic scored his third goal in a...   \n",
       "3   Nobel laureate Norman E. Borlaug , an agricul...   \n",
       "4   Louisiana Gov. Bobby Jindal on Monday stood b...   \n",
       "\n",
       "                                           highlight  \n",
       "0  [U.S.-based scientists say their data points t...  \n",
       "1  [Zimmerman posts $ 5,000 bail ; he was accused...  \n",
       "2  [Barcelona move three points clear of Real Mad...  \n",
       "3  [Borlaug died at the age of 95 from complicati...  \n",
       "4  [Louisiana Gov. Bobby Jindal decried `` no-go ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cnn = pd.DataFrame({'article_id':vec_ids,'story':vec_stories,'highlight':vec_highlights})\n",
    "print(df_cnn.shape)\n",
    "df_cnn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a040f3-489e-4cc9-b615-a891cba571bc",
   "metadata": {},
   "source": [
    "It was observed that the following article IDs do not have stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f69ff18-f8a3-4e5c-a2b9-ec2c2dcf9583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 226ca83313bb4db0917847f80fcf4a2d2af5007d\n",
      "- c36fb222cee4c1f4e38cf62ad37e2eb8dd0a85be\n",
      "- d4b4ee22583e0490d5e41e93941e8e6ec182d7ab\n",
      "- 2cb398794fea7b2dd83501c401c034ca73362323\n"
     ]
    }
   ],
   "source": [
    "print('- '+'\\n- '.join(df_cnn.loc[df_cnn['story']=='']['article_id'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd6b876a-7ce3-4abe-82b1-11249ec6769d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 5001/5001 [25:59<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 44s, sys: 14.3 s, total: 25min 58s\n",
      "Wall time: 25min 59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Setup storage for iteration results...\n",
    "val_cnt = 0\n",
    "vec_iter_article_id = []\n",
    "vec_iter_sent_ids = []\n",
    "vec_iter_sent_scores = []\n",
    "vec_iter_sent_strs = []\n",
    "vec_iter_rouge = []\n",
    "vec_iter_method = []\n",
    "\n",
    "#ROUGE scoring function\n",
    "rouge_scr = rouge_scorer.RougeScorer(['rouge1','rougeL'],use_stemmer=True)\n",
    "\n",
    "#Create iterations over test conditions...\n",
    "for i,val_row in tqdm(df_cnn.iterrows(),total=df_cnn.shape[0],miniters=10):\n",
    "    if(not val_row['story']==''):\n",
    "        for func_summary_method in [None,np.mean]:\n",
    "            #Ensure the method is generating the same number of sentences as the highlight...\n",
    "            val_num_sents = len(val_row['highlight'])\n",
    "            luhn_rtn = la.luhn_abstract.run_auto_summarization(val_text=val_row['story'],\n",
    "                                                               val_n=val_num_sents,\n",
    "                                                               is_sw_remove=False,\n",
    "                                                               is_sw_zero=False,\n",
    "                                                               is_use_luhn_tf=True,\n",
    "                                                               vec_sw_luhn=la.luhn_abstract.vec_luhn_sw,\n",
    "                                                               func_stem_selected=la.luhn_abstract.tokenize_stem_nltk,\n",
    "                                                               func_summary_selected=func_summary_method,\n",
    "                                                               is_print=False)\n",
    "            if(luhn_rtn[1].shape[0]>0):\n",
    "                vec_iter_article_id.append(val_row['article_id'])\n",
    "                vec_iter_method.append(func_summary_method.__name__ if func_summary_method else 'None')\n",
    "                vec_tmp_ids = luhn_rtn[1]['id_sent'].iloc[0:val_num_sents].values.tolist()\n",
    "                vec_iter_sent_ids.append(vec_tmp_ids)\n",
    "                vec_iter_sent_scores.append(luhn_rtn[2])\n",
    "                vec_iter_sent_strs.append(luhn_rtn[3])\n",
    "                vec_iter_rouge.append(rouge_scr.score(target=' '.join(val_row['highlight']),prediction=' '.join(luhn_rtn[3])))\n",
    "                del(vec_tmp_ids)\n",
    "        val_cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1270229f-bbba-463e-bc44-d400aa935ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This resulted in 5,001 iterations; only 9,994 summaries were generated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>vec_sent_ids</th>\n",
       "      <th>vec_scores</th>\n",
       "      <th>vec_sents</th>\n",
       "      <th>func_summary_method</th>\n",
       "      <th>rouge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>638ba1352bdf405a8f5bd681d7fe5c928686afff</td>\n",
       "      <td>[15.0, 1.0, 3.0, 0.0]</td>\n",
       "      <td>[7.5625, 6.32258064516129, 5.76, 4.76470588235...</td>\n",
       "      <td>[`` We now have more than double the data we h...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'rouge1': (0.22448979591836735, 0.61111111111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>638ba1352bdf405a8f5bd681d7fe5c928686afff</td>\n",
       "      <td>[15.0, 1.0, 3.0, 16.0]</td>\n",
       "      <td>[3.8823529411764706, 3.3870967741935485, 3.12,...</td>\n",
       "      <td>[`` We now have more than double the data we h...</td>\n",
       "      <td>mean</td>\n",
       "      <td>{'rouge1': (0.2125984251968504, 0.5, 0.2983425...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f9f9601180ab3278165d936821e8f145659997f3</td>\n",
       "      <td>[10.0, 12.0, 6.0]</td>\n",
       "      <td>[14.08695652173913, 12.041666666666666, 10.888...</td>\n",
       "      <td>[In fact , it 's his second arrest for alleged...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'rouge1': (0.10869565217391304, 0.22727272727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f9f9601180ab3278165d936821e8f145659997f3</td>\n",
       "      <td>[10.0, 12.0, 6.0]</td>\n",
       "      <td>[6.576923076923077, 6.375, 5.526315789473684]</td>\n",
       "      <td>[In fact , it 's his second arrest for alleged...</td>\n",
       "      <td>mean</td>\n",
       "      <td>{'rouge1': (0.10869565217391304, 0.22727272727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>80ec0efb252ec4470aee44482d1e196111b5780b</td>\n",
       "      <td>[16.0, 12.0, 1.0, 13.0]</td>\n",
       "      <td>[6.722222222222222, 6.722222222222222, 5.78571...</td>\n",
       "      <td>[The win lifted Zaragoza four points clear of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'rouge1': (0.27419354838709675, 0.55737704918...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 article_id             vec_sent_ids  \\\n",
       "0  638ba1352bdf405a8f5bd681d7fe5c928686afff    [15.0, 1.0, 3.0, 0.0]   \n",
       "1  638ba1352bdf405a8f5bd681d7fe5c928686afff   [15.0, 1.0, 3.0, 16.0]   \n",
       "2  f9f9601180ab3278165d936821e8f145659997f3        [10.0, 12.0, 6.0]   \n",
       "3  f9f9601180ab3278165d936821e8f145659997f3        [10.0, 12.0, 6.0]   \n",
       "4  80ec0efb252ec4470aee44482d1e196111b5780b  [16.0, 12.0, 1.0, 13.0]   \n",
       "\n",
       "                                          vec_scores  \\\n",
       "0  [7.5625, 6.32258064516129, 5.76, 4.76470588235...   \n",
       "1  [3.8823529411764706, 3.3870967741935485, 3.12,...   \n",
       "2  [14.08695652173913, 12.041666666666666, 10.888...   \n",
       "3      [6.576923076923077, 6.375, 5.526315789473684]   \n",
       "4  [6.722222222222222, 6.722222222222222, 5.78571...   \n",
       "\n",
       "                                           vec_sents func_summary_method  \\\n",
       "0  [`` We now have more than double the data we h...                None   \n",
       "1  [`` We now have more than double the data we h...                mean   \n",
       "2  [In fact , it 's his second arrest for alleged...                None   \n",
       "3  [In fact , it 's his second arrest for alleged...                mean   \n",
       "4  [The win lifted Zaragoza four points clear of ...                None   \n",
       "\n",
       "                                               rouge  \n",
       "0  {'rouge1': (0.22448979591836735, 0.61111111111...  \n",
       "1  {'rouge1': (0.2125984251968504, 0.5, 0.2983425...  \n",
       "2  {'rouge1': (0.10869565217391304, 0.22727272727...  \n",
       "3  {'rouge1': (0.10869565217391304, 0.22727272727...  \n",
       "4  {'rouge1': (0.27419354838709675, 0.55737704918...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cnn_msmts = pd.DataFrame({'article_id':vec_iter_article_id,'vec_sent_ids':vec_iter_sent_ids,'vec_scores':vec_iter_sent_scores,\n",
    "                             'vec_sents':vec_iter_sent_strs,'func_summary_method':vec_iter_method,'rouge':vec_iter_rouge})\n",
    "print(f'''This resulted in {val_cnt:,} iterations; only {df_cnn_msmts.shape[0]:,} summaries were generated''')\n",
    "df_cnn_msmts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6336f52f-e4d3-41fc-acdc-19033b586fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##KDE Plots....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e1152-9fad-44a5-ade8-39a3e5a75808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd0b44-01ee-4464-82fc-095e03eeef63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff1921-ba65-4d44-a574-971b46ff1926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f531806-46ef-4982-b84f-24fcc1d929a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5362e-1066-49c5-9037-68b0f88bfadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736290a9-9858-4b18-b1a6-e4b9ea6620ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be87ee-9ff0-492e-832d-6deefe0dc20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e4c968-e927-429f-9521-cd1f041c499a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b4d3a1-73b4-4b62-b3bc-3e34668c739f",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Original Notebook\n",
    "## Imports/Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b62d2001-874a-4008-94f0-f33887f45074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc727778-79ef-4ddb-8725-c597bca19ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page_wikipedia(val_url):\n",
    "    html_scraped_text = requests.get(val_url)\n",
    "    html_text_parsed = bs.BeautifulSoup(html_scraped_text.content,'lxml')\n",
    "    html_text_content = html_text_parsed.find(id='content')\n",
    "    html_text_paragraphs = html_text_content.find_all('p')\n",
    "    val_text_formatted = ''\n",
    "    for val in html_text_paragraphs:\n",
    "        val_text_formatted += val.text\n",
    "    return(val_text_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18097d6-c67e-4a74-a497-124fd7d15820",
   "metadata": {},
   "source": [
    "## Examples\n",
    "### Request NLP Wikipedia Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "91f47de3-6708-46be-b423-ce009c4c8d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 111 ms, sys: 10.9 ms, total: 122 ms\n",
      "Wall time: 488 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/natural_language_processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f3102-82a1-4769-9e8c-f3ad49e5930d",
   "metadata": {},
   "source": [
    "#### Experiment #1: Use Luhn Score\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa3ca42c-7523-49af-9875-308d3b61abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 31\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[22.5000]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[18.0000]\u001b[0m The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. \u001b[31m[17.4222]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[16.2881]\u001b[0m\n",
      "CPU times: user 551 ms, sys: 18 ms, total: 569 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp1 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67502dcf-706f-4397-b6e7-4a05aa429b51",
   "metadata": {},
   "source": [
    "#### Experiment #2: Remove Stop Words\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| Yes | True |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2511b05f-48e4-4e33-a0e2-9c00409f009c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 8\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[12.0417]\u001b[0m In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. \u001b[31m[8.0667]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[7.7576]\u001b[0m [9] In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. \u001b[31m[7.5789]\u001b[0m\n",
      "CPU times: user 238 ms, sys: 5.83 ms, total: 243 ms\n",
      "Wall time: 242 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp2 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=True,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f86b7d-dce1-459f-bc8c-b2ae42a3234a",
   "metadata": {},
   "source": [
    "#### Experiment #3: Use Mean Score\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36991b12-1c23-497d-8740-6e18c8e553c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 31\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[10.1087]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[8.7736]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[8.4068]\u001b[0m The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. \u001b[31m[8.2857]\u001b[0m\n",
      "CPU times: user 549 ms, sys: 12.5 ms, total: 562 ms\n",
      "Wall time: 553 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp3 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=np.mean,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd73e984-65d8-49de-9eb9-34a4339f4262",
   "metadata": {},
   "source": [
    "#### Experiment #4: Zero Score Stop Words\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | Luhn Counting | True |\n",
    "| Stop Words Removed from Score | Yes | True |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f2e42f1-f062-42fc-97fb-bcab9aadbf19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0\n",
      "Quantile Significance Upper = 31\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[29.8983]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[21.4912]\u001b[0m [17] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming. \u001b[31m[19.6122]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[15.3725]\u001b[0m\n",
      "CPU times: user 651 ms, sys: 9.77 ms, total: 661 ms\n",
      "Wall time: 655 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp4 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=True,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f86b663-8ff0-4920-b9b5-e5edf9fd9642",
   "metadata": {},
   "source": [
    "#### Experiment #4: Use TF Counting w/Mean Score\n",
    "\n",
    "$$tf=\\frac{f_t}{\\sum{f_t}}$$\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ddb09da-ad85-41aa-8dd3-987491a8df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0\n",
      "Quantile Significance Upper = 0\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[10.1087]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[8.7736]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[8.4068]\u001b[0m The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. \u001b[31m[8.2857]\u001b[0m\n",
      "CPU times: user 574 ms, sys: 8.14 ms, total: 582 ms\n",
      "Wall time: 577 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp5 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=False,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=np.mean,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da013bdf-a2c1-4ea2-ad02-2ce1b60da858",
   "metadata": {},
   "source": [
    "#### Experiment #5: Use TF Counting\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | No | False |\n",
    "| Aggregation Function | Luhn Algorithm | None |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9e5df40-073a-425f-8a2a-5b2e66a02d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0\n",
      "Quantile Significance Upper = 0\n",
      "Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:  Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \u001b[31m[22.5000]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[18.0000]\u001b[0m The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts. \u001b[31m[17.4222]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[16.2881]\u001b[0m\n",
      "CPU times: user 585 ms, sys: 11.8 ms, total: 597 ms\n",
      "Wall time: 591 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp5 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=False,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a399aa17-9a00-451c-81cd-9eb5a9480f4c",
   "metadata": {},
   "source": [
    "#### Experiment #5: Use TF Counting w/Stop Words Zeroed\n",
    "\n",
    "| Parameter | Value (English) | Value (Set) |\n",
    "| :-: | :-: | :-: |\n",
    "| Stop Words Removed from Text| No | False |\n",
    "| Counting Method | TF Counting | False |\n",
    "| Stop Words Removed from Score | Yes | True |\n",
    "| Aggregation Function | Mean | np.mean |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d441466-b329-4377-97d4-57263fa8043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 0\n",
      "Quantile Significance Upper = 0\n",
      "[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston. \u001b[31m[15.3051]\u001b[0m As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects: Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. \u001b[31m[10.6780]\u001b[0m [17] Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming. \u001b[31m[9.9200]\u001b[0m [8] In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors. \u001b[31m[7.6604]\u001b[0m\n",
      "CPU times: user 665 ms, sys: 18 ms, total: 683 ms\n",
      "Wall time: 669 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "luhn_rtn_exp5 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=True,\n",
    "                                                        is_use_luhn_tf=False,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=np.mean,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60dc2d-b8c9-4914-876e-ab86fd089720",
   "metadata": {},
   "source": [
    "### Request George Washington Wikipedia Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1a7bf20d-3dcb-4dc8-b95a-ff29f2b095a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 17.8 ms, total: 288 ms\n",
      "Wall time: 451 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/George_Washington')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "376f06c3-5efb-42ff-b02c-5912bfd1e145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 2\n",
      "Quantile Significance Upper = 471\n",
      "He also maintains that Washington never advocated outright confiscation of tribal land or the forcible removal of tribes and that he berated American settlers who abused natives, admitting that he held out no hope for peaceful relations as long as \"frontier settlers entertain the opinion that there is not the same crime (or indeed no crime at all) in killing a native as in killing a white man. \u001b[31m[44.4853]\u001b[0m While his relationship with Washington would remain friendly, Washington's relationship with his Secretary of War Henry Knox deteriorated after rumors that Knox had profited from contracts for the construction of U.S. frigates which had been commissioned under the Naval Act of 1794 in order to combat Barbary pirates, forcing Knox to resign. \u001b[31m[35.8519]\u001b[0m Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on \"a shore foreign to his native soil\" if the country became divided, and Washington's remains stayed in Mount Vernon. \u001b[31m[33.8000]\u001b[0m [38] Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor as Commander-in-Chief, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. \u001b[31m[33.0652]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "luhn_rtn_exp6 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cce7f0-b43c-4790-a06c-910bb05753b0",
   "metadata": {},
   "source": [
    "### Request GMU Wikipedia Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fcee509-d839-4c70-9cb1-29c9aed9a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = scrape_page_wikipedia(val_url='https://en.wikipedia.org/wiki/George_Mason_University')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45db3c12-4ed3-4641-8d37-e80bcc2df464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 127\n",
      "Among undergraduate students, 80% of students are enrolled full-time while 20% are enrolled part-time[146] In terms of ethnic and racial demographics: American Indian/Alaska Native people make up 0% of the student body and 0% of the full-time staff; Asian people make up 22% of the student body and 14% of the full-time staff; Black people make up 11% of the student body and 5% of the full-time staff; Hispanic and Latino people make up 17% of the student population and 3% of the full-time staff; Native Hawaiian/Pacific Islander people make up 0% of the student body and 0% of the full-time staff; non-resident alien people make up 5% of the student body and 8% of the full-time staff; people of two or more races/multiracial people make up 5% of the student body and 2% of the full-time staff; people of an Unknown ethno-racial demographic make up 3% of the student body and 3% of the full-time staff; and White people make up 36% of the student body and 65% of the full-time staff. \u001b[31m[73.2842]\u001b[0m 8 in the nation for Freedom of Speech and protecting rights enshrined in the First Amendment to the United States Constitution according to the Foundation for Individual Rights and Expression (FIRE), FIRE also posits that the majority viewpoint of the student body leans politically liberal in the sense of modern liberalism in the United States,[138][139] although the political ideologies of libertarianism in the United States[140][141][142] and conservatism in the United States[143][144] are also visible on campus with the university stating that it strives for \"comprehensive ideological balance,\" evidence including but not limited to the university being \"home to both the Antonin Scalia Law School and the Jimmy and Rosalynn Carter School for Peace and Conflict Resolution,\" named after a conservative U.S. Supreme Court Justice (Antonin Scalia) politically appointed by Republican Party U.S. President and a liberal U.S. President (Jimmy Carter) and First Lady (Rosalynn Carter) who are members of the Democratic Party, respectively. \u001b[31m[70.6728]\u001b[0m [147] The naming of the Antonin Scalia Law School after the late conservative United States Supreme Court Justice Antonin Scalia, the hiring of conservative United States Supreme Court Justices Bret Kavanaugh, Neil Gorsuch, and Clarence Thomas as professors, the allegedly “lavish treatments,” speaking gigs, and \"all-experiences-paid\" travel arrangements they received, its close ties with the  Federalist Society for Law and Public Policy Studies, and the extensive provision of professional development and continuing education programs, as well as speaking engagements for sitting judges of lower and appellate divisions - in particular dealing with the topic of law and economics - has brought on controversy on the university itself, the Supreme Court of the United States, and the Federal Judiciary of the United States as a whole over the overt conservative political influence taking place at the law school and the university's growing influence over the U.S. judicial system. \u001b[31m[44.7552]\u001b[0m [a] Notable buildings include the 320,000-square-foot (30,000 m2) student union building, the Johnson Center; the Center for the Arts, a 2,000-seat concert hall; the 180,000-square-foot (17,000 m2) Long and Kimmy Nguyen Engineering Building; Exploratory Hall for science, new in 2013; an astronomy observatory and telescope; the 88,900-square-foot (8,260 m2) Art and Design Building; the newly expanded Fenwick Library,[49] the Krasnow Institute; and three fully appointed gyms and an aquatic center for student use. \u001b[31m[33.1364]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "luhn_rtn_exp7 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a79cd-64e4-40c8-bb78-75f19d82d469",
   "metadata": {},
   "source": [
    "## Random Example\n",
    "\n",
    "*Source: Scientific American*  \n",
    "*Title: A New Quantum Cheshire Cat Thought Experiment Is Out of the Box*  \n",
    "*Author: Manon Bischoff*\n",
    "\n",
    "[A New Quantum Cheshire Cat Thought Experiment Is Out of the Box](https://www.scientificamerican.com/article/a-new-quantum-cheshire-cat-thought-experiment-is-out-of-the-box/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "494a2d7a-d103-495b-b827-9c513311aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = '''Physicists seem to be obsessed with cats. James Clerk Maxwell, the father of electrodynamics, studied falling felines to investigate how they turned as they fell. Many physics teachers have used a cat’s fur and a hard rubber rod to explain the phenomenon of frictional electricity. And Erwin Schrödinger famously illustrated the strangeness of quantum physics with a thought experiment involving a cat that is neither dead nor alive.  So it hardly seems surprising that physicists turned to felines once again to name a newly discovered quantum phenomenon in a paper published in the New Journal of Physics in 2013. Their three-sentence study abstract reads, “In this paper we present a quantum Cheshire Cat. In a pre- and post-selected experiment we find the Cat in one place, and its grin in another. The Cat is a photon, while the grin is its circular polarization.”\n",
    "The newfound phenomenon was one in which certain particle features take a different path from their particle—much like the smile of the Cheshire Cat in Alice’s Adventures in Wonderland, written by Lewis Carroll—a pen name of mathematician Charles Lutwidge Dodgson—and published in 1865. To date, several experiments have demonstrated this curious quantum effect. But the idea has also drawn significant skepticism. Critics are less concerned about the theoretical calculations or experimental rigor than they are about the interpretation of the evidence. “It seems a bit bold to me to talk about disembodied transmission,” says physicist Holger Hofmann of Hiroshima University in Japan. “Instead we should revise our idea of particles.”  Recently researchers led by Yakir Aharonov of Chapman University took the debate to the next level. Aharonov was a co-author of the first paper to propose the quantum Cheshire effect. Now, on the preprint server arXiv.org, he and his colleagues have posted a description of theoretical work that they believe demonstrates that quantum properties can move without any particles at all—like a disembodied grin flitting through the world and influencing its surroundings—in ways that bypass the critical concerns raised in the past.\n",
    "Aharonov and his colleagues first encountered their quantum Cheshire cat several years ago as they were pondering one of the most fundamental principles of quantum mechanics: nothing can be predicted unambiguously. Unlike classical physics, the same quantum mechanical experiment can have different outcomes under exactly the same conditions. It is therefore impossible to predict the exact outcome of a single experiment—only its outcome with a certain probability. “Nobody understands quantum mechanics. It’s so counterintuitive. We know its laws, but we are always surprised,” says Sandu Popescu, a physicist at the University of Bristol in England, who collaborated with Aharonov on the 2013 paper and the new preprint.\n",
    "But Aharonov was not satisfied with this uncertainty. So, since the 1980s, he has been exploring ways to investigate fundamental processes despite the probability-based nature of quantum mechanics. Aharonov—now age 92—employs an approach that involves intensively repeating an experiment, grouping results and then examining what came out before and after the experiment and relating these events to each other. “To do this, you have to understand the flow of time in quantum mechanics,” Popescu explains. “We developed a completely new method to combine information from measurements before and after the experiment.”\n",
    "The researchers have stumbled across several surprises with this method—including their theoretical Cheshire cat. Their idea sounds simple at first: send particles through an optical tool called an interferometer, which causes each particle to move through one of two paths that ultimately merge again at the end. If the setup and measurements were carried out skillfully, Aharonov and his colleagues theorized, it could be shown that the particle traveled a path in the interferometer that differed from the path of its polarization. In other words, they claimed the property of the particle could be measured on one path even though the particle itself took the other—as if the grin and the cat had come apart.\n",
    "Inspired by this theory, a team led by Tobias Denkmayr, then at the Vienna University of Technology, implemented the experiment with neutrons in a study published in 2014. The team showed that the neutral particles inside an interferometer followed a different path from that of their spin, a quantum mechanical property of particles similar to angular momentum: Denkmayr and his colleagues had indeed found evidence of the Cheshire cat theory. Two years later researchers led by Maximilian Schlosshauer of the University of Portland successfully implemented the same experiment with photons. The scientists saw evidence that the light particles took a different path in the interferometer than their polarization did.\n",
    "But not everyone is convinced. “Such a separation makes no sense at all. The location of a particle is itself a property of the particle,” Hofmann says. “It would be more accurate to talk about an unusual correlation between location and polarization.” Last November Hofmann and his colleagues provided an alternative explanation based on widely known quantum mechanical effects.\n",
    "And in another interpretation of the Cheshire cat results, Pablo Saldanha of the Federal University of Minas Gerais in Brazil and his colleagues argue that the findings can be explained with wave-particle duality. “If you take a different view, there are no paradoxes,” Saldanha says, “but all results can be explained with traditional quantum mechanics as simple interference effects.”\n",
    "Much of the controversy surrounds the way in which particles’ properties and positions are detected in these experiments. Disturbing a particle could alter its quantum mechanical properties. For that reason, the photons or neutrons cannot be recorded inside the interferometer using an ordinary detector. Instead scientists must resort to a principle of weak measurement developed by Aharonov in 1988. A weak measurement makes it possible to scan a particle very lightly without destroying its quantum state. This comes at a price, however: the weak measurement result is extremely inaccurate. (Thus, these experiments must be repeated many times over, to compensate for the fact that each individual measurement is highly uncertain.)\n",
    "In the quantum Cheshire cat experiments, a weak measurement is made along a path in the interferometer, the paths then merge, and the emerging particles are measured with an ordinary detector. Along one path of the interferometer, a weak measurement of the particle’s position can be taken and, along the other, its spin. Using detectors, physicists can more definitively characterize the particles that traveled through the interferometer and potentially reconstruct what occurred during the particle’s journey. For example, only certain particles will appear in certain detectors, helping the physicists piece together which path their neutron or photon previously took. According to Aharonov, Popescu and their colleagues, the Cheshire cat experiments ultimately reveal that the particle’s position can be confirmed on one path even as its polarization or spin was measured on the other.\n",
    "Saldanha and his co-authors assert that it is impossible to make claims about quantum systems in the past given their measurements in the present. In other words, the photons and neutrons measured in the final detectors cannot tell us much about their previous trajectory. Instead the wave functions of particles passing through the paths of the interferometer could overlap, which would make it impossible to trace which path a particle had taken. “Ultimately, the paradoxical behaviors are related to the wave-particle duality,” Saldanha says. But in the papers that report evidence of the quantum Cheshire cat, he asserts, the findings “are processed in a sophisticated way that obscures this simpler interpretation.”\n",
    "Hofmann, meanwhile, has stressed that the results will differ if you measure the system in a different way. This phenomenon is well-known in quantum physics: if, for example, you first measure the speed of a particle and then its position, the result can be different than it would be if you first measured the position of the same particle and then its speed. He and his colleagues therefore contend that Aharonov and his team’s conclusions were correct in themselves—that the particle moved along one path and the polarization followed the other—but that such differing paths do not apply simultaneously.\n",
    "As Hofmann’s co-author Jonte Hance, also at Hiroshima University, told New Scientist, “It only looks like [the particle and polarization are] separated because you’re measuring one of the properties in one place and the other property in the other place, but that doesn’t mean that the properties are in one place and the other place, that means that the actual measuring itself is affecting it in such a way that it looks like it’s in one place and the other place.”\n",
    "But these critiques are “missing the point,” Popescu says. He agrees that the work and reasoning put forward by Saldanha and Hofmann’s respective groups are correct—but adds that the best way to test any interpretation is to generate testable predictions from each. “As I understand it, there is no direct way to make predictions based on them,” Popescu says in reference to these alternative explanations. “They kind of have a very old-fashioned way of looking at things: there are contradictions, so you stop doing the math.”\n",
    "With their recent preprint paper, Aharonov and Popescu, together with physicist Daniel Collins of the University of Bristol, have now described how a particle’s spin can move completely independently of the particle itself—without employing a weak measurement. In their new experimental setup, a particle is located in the left half of an elongated two-part cylinder that is sealed at the outer edges. Because of a highly reflective wall in the middle, the particle has a vanishingly small probability of tunneling through to the right-hand side of the cylinder. In their paper, the researchers provide a proof that even if the particle remains in the left-hand area in almost all cases, it should still be possible to measure a transfer of the particle’s spin at the right-hand outer wall. “It’s amazing, isn’t it?” Collins says. “You think the particle has a spin and the spin should stay with the particle. But the spin crosses the box without the particle.”\n",
    "This approach would address several of the critical concerns raised thus far. The physicists don't need weak measurements. Nor do they need to group their experimental results to draw temporal conclusions. (That being said, grouping results would still improve the measurements, given that the angular momentum of the wall itself cannot be determined unambiguously because of the Heisenberg uncertainty principle.) But in this scenario, the only physical principles involved are conservation laws, such as the conservation of energy or the conservation of momentum and angular momentum. Popescu and Collins explain that they hope other groups will implement the experiment to observe the effects in the laboratory.\n",
    "The new work has piqued Hofmann’s interest. “The scenario is exciting because the interaction between polarization and particle motion produces a particularly strong quantum effect that clearly contradicts the particle picture,” he says.\n",
    "But he still does not see this as proof of disembodied (particle-free) spin transfer. “For me, this means, above all, that it is wrong to assume a measurement-independent reality,” Hofmann says. Instead quantum mechanics allows a particle’s residence to extend to the right-hand region of the cylinder, even if a residence in the left-hand region seems logically compelling. “I think it is quite clear to Aharonov, Collins and Popescu that the space in front of the wall is not really empty,” he adds.\n",
    "Saldanha, meanwhile, still sees the researchers as overcomplicating what could be explained as traditional quantum interference effects. When discussing the particle’s very low probability of entering the right-hand side of the experimental setup, he explains, “we have to be careful about a ‘vanishingly small probability’ when we refer to waves.” The wave function of the particle could also expand into the right-hand side of the setup and thus influence the angular momentum of the wall. “The same predictions can be made without such dramatic conclusions,” he says.\n",
    "In response to these critiques, Popescu says, “This is of course another way of thinking about it. The question is whether this interpretation is useful.” Regardless of which interpretation of the events is correct, the quantum Cheshire cat could enable new technological applications. For example, it could be used to transfer information or energy without moving a physical particle—whether made of matter or light.\n",
    "For Popescu, however, the fundamental questions of physics play a more important role. “It all started when we thought about how time propagates in quantum mechanics,” he says. “And suddenly we were able to discover something fundamental about the laws of conservation.”\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0edd49a4-4d65-48d9-80ac-f335e7ee086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Significance Lower = 1\n",
      "Quantile Significance Upper = 56\n",
      "As Hofmann’s co-author Jonte Hance, also at Hiroshima University, told New Scientist, “It only looks like [the particle and polarization are] separated because you’re measuring one of the properties in one place and the other property in the other place, but that doesn’t mean that the properties are in one place and the other place, that means that the actual measuring itself is affecting it in such a way that it looks like it’s in one place and the other place.” But these critiques are “missing the point,” Popescu says. \u001b[31m[54.5684]\u001b[0m This phenomenon is well-known in quantum physics: if, for example, you first measure the speed of a particle and then its position, the result can be different than it would be if you first measured the position of the same particle and then its speed. \u001b[31m[31.3913]\u001b[0m “If you take a different view, there are no paradoxes,” Saldanha says, “but all results can be explained with traditional quantum mechanics as simple interference effects.” Much of the controversy surrounds the way in which particles’ properties and positions are detected in these experiments. \u001b[31m[26.2727]\u001b[0m According to Aharonov, Popescu and their colleagues, the Cheshire cat experiments ultimately reveal that the particle’s position can be confirmed on one path even as its polarization or spin was measured on the other. \u001b[31m[24.7353]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "luhn_rtn_exp8 = la.luhn_abstract.run_auto_summarization(val_text=val_text,\n",
    "                                                        is_sw_remove=False,\n",
    "                                                        is_sw_zero=False,\n",
    "                                                        is_use_luhn_tf=True,\n",
    "                                                        vec_sw_luhn=[],\n",
    "                                                        vec_sw_add=[],\n",
    "                                                        func_stem_selected=None,\n",
    "                                                        func_summary_selected=None,\n",
    "                                                        is_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6ee875-3665-4d41-8473-27c9b59c4486",
   "metadata": {},
   "source": [
    "## Potential Issues...\n",
    "\n",
    "- Parsing Wikipedia with NLTK results in sentences that are sometimes more than one independent clause/sentence.\n",
    "- Reproducing the results from the paper is complicated by the fact that exact values for *C* and *D* are not documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51e17b-cf4f-49eb-a2c2-be5580b32d57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
